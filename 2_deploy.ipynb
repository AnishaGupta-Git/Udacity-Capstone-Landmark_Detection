{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "98bd640050a53c22886899003f977aff23e523f614083c0f5c0fe214761a6263"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This file assumes that we have the following dependencies available:\n",
    "* *.pt : PyTorch saved model file\n",
    "* \"description.json\" : Has information about all the landmark classes\n",
    "* \"thresholds.json\" : Has information about the minimum confidence level for prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import json\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import requests\n",
    "import io\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MODEL = \"./deploy/resnet34_bs-128_lr-3e3_ep-50.pt\"\n",
    "CLASS_LABELS =  ['Auroville', 'Buddhist Sanchi', 'Charminar', 'Chhatrapati Shivaji Terminus', 'Dakshineshwar', 'Gateway Of India', 'Golden Temple', 'Hampi', 'Hawa Mahal', 'Howrah Bridge', 'Humayun Tomb', 'India Gate', 'Jagannath Puri', 'Jantar Mantar', 'Jog Falls', 'Kanchenjunga', 'Lotus Temple', 'Meenakshi Temple', 'Mysore Palace', 'Qutub Minar', 'Red Fort', 'Sun Temple', 'Taj Mahal', 'Victoria Memorial', 'Wagah Border'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = json.load(open(\"./deploy/description.json\",\"r\"))\n",
    "thresholds = json.load(open(\"./deploy/thresholds.json\",\"r\"))\n",
    "\n",
    "# Make sure all class names are all right!\n",
    "assert set(description) == set(thresholds)\n",
    "assert set(CLASS_LABELS) == set(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads an image from img_path into a PIL.Image\n",
    "# img_path: Can be a URL or a local file path\n",
    "def img_path_to_data(img_path):\n",
    "    # Check if the path is an image URL or local file\n",
    "    if \"http\" in img_path or \"ftp\" in img_path or \"www\" in img_path:\n",
    "        response = requests.get(img_path)\n",
    "        img_data = PIL.Image.open(io.BytesIO(response.content))\n",
    "    else:\n",
    "        img_data = PIL.Image.open(img_path)\n",
    "    \n",
    "    # Convert to RGB for images with 4 layers like PNGs\n",
    "    return img_data.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_data: PIL.Image format image\n",
    "# model: torch.model\n",
    "# Returns a tuple of the class_id [0,24] with maximum probability and a torch.tensor of probabilities of all class_id's\n",
    "def predict_on_img_data(img_data, model):\n",
    "    test_transform = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.485, 0.456, 0.406],  # Recommended values for ImageNet trained models\n",
    "                    [0.229, 0.224, 0.225])\n",
    "                ])\n",
    "        \n",
    "    img_tensor = test_transform(img_data).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        proba = torch.nn.functional.softmax(model(img_tensor)).flatten()\n",
    "    class_id = torch.argmax(proba).item()\n",
    "    return (class_id,proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function, called by front-end UI framework\n",
    "def predict(img):\n",
    "    class_id, pred = predict_on_img_data(img,model)\n",
    "    class_label = CLASS_LABELS[class_id]\n",
    "    pred_float = {CLASS_LABELS[i]:float(pred[i]) for i in range(len(pred))}\n",
    "\n",
    "    desc = description[class_label]\n",
    "    if pred_float[class_label] < thresholds[class_label]:\n",
    "        desc = '<p style=\"color:rgb(255, 0, 0);\"><b>The model is not very confident about this prediction. Maybe this is something unfamiliar or confusing to it!</b></p>'\n",
    "    return pred_float, desc"
   ]
  },
  {
   "source": [
    "# Create front-end user interface"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(PATH_MODEL).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_title = \"Indian Landmark Detection\"\n",
    "ui_desc = \"Capstone Project by Anisha Gupta for Udacity MLE-Nanodegree, October 2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running locally at: http://127.0.0.1:7860/\nTo get a public link for a hosted model, set Share=True\nInterface loading below...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x2881e95c348>",
      "text/html": "\n        <iframe\n            width=\"1000\"\n            height=\"500\"\n            src=\"http://127.0.0.1:7860/\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<Flask 'gradio.networking'>, 'http://127.0.0.1:7860/', None)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "front_end = gr.Interface(fn= predict,\n",
    "            inputs=    gr.inputs.Image(type=\"pil\", label=\"Select image to check\"),\n",
    "            outputs=   [gr.outputs.Label(type=\"confidences\", num_top_classes=3, label= \"Predicted Landmark\"),\n",
    "                        gr.outputs.HTML(label= \"Description\")],\n",
    "            title=      ui_title,\n",
    "            description= ui_desc,\n",
    "            allow_flagging= False)\n",
    "front_end.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}